{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Deep Learning - Text Classification - Reuters Articles - Keras\n",
    "\n",
    "\n",
    "This notebook explores the [Keras](https://keras.io/) layers with embedding mechanism on reuters articles data-set (smaller version).\n",
    "Moreover, it uses keras provided packages for sequesntial modeling and text preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Tasks \n",
    "\n",
    "Classifying tasks for the dataset follow below steps:\n",
    "- Figuring out the model which is suitable for the given data.\n",
    "- Complete layers representation with suitable loss functions.\n",
    "- Experimenting, validating and evaluating different models.\n",
    "- Briefly document few best models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import keras\n",
    "from keras.datasets import reuters\n",
    "import glob\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.utils.data_utils import get_file\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout\n",
    "from keras.models import Model, Sequential\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "This time our dataset is a dataset of 11,228 newswires from Reuters, labeled over 46 topics. Each wire is encoded as a sequence of word indexes (same conventions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(path=\"reuters.npz\",\n",
    "                                                         num_words=None,\n",
    "                                                         skip_top=0,\n",
    "                                                         maxlen=None,\n",
    "                                                         test_split=0.2,\n",
    "                                                         seed=113,\n",
    "                                                         start_char=1,\n",
    "                                                         oov_char=2,\n",
    "                                                         index_from=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the data is formatted by printing the dimensionalities of the variables. Refer to the keras-documentation for further info: https://keras.io/datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x_train size\", x_train.shape)\n",
    "print(\"y_train size\", y_train.shape)\n",
    "print(\"x_test size\", x_test.shape)\n",
    "print(\"y_test size\", y_test.shape)\n",
    "print(\"\")\n",
    "print(\"Number of classes:\", np.unique(y_train).shape[0])\n",
    "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using only dataset features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing sequence data...\n",
      "x_train shape: (8982, 4000)\n",
      "x_test shape: (2246, 4000)\n",
      "Convert class vector to binary class matrix (for use with categorical_crossentropy)\n",
      "y_train shape: (8982, 46)\n",
      "y_test shape: (2246, 46)\n"
     ]
    }
   ],
   "source": [
    "MAX_NB_WORDS = 4000\n",
    "\n",
    "num_classes = np.unique(y_train).shape[0]\n",
    "print('Vectorizing sequence data...')\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
    "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Convert class vector to binary class matrix '\n",
    "      '(for use with categorical_crossentropy)')\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Train on 8083 samples, validate on 899 samples\n",
      "Epoch 1/10\n",
      "8083/8083 [==============================] - 2s - loss: 1.7271 - acc: 0.6343 - val_loss: 1.1771 - val_acc: 0.7486\n",
      "Epoch 2/10\n",
      "8083/8083 [==============================] - 2s - loss: 0.8163 - acc: 0.8220 - val_loss: 0.9709 - val_acc: 0.7831\n",
      "Epoch 3/10\n",
      "8083/8083 [==============================] - 2s - loss: 0.5188 - acc: 0.8895 - val_loss: 0.8764 - val_acc: 0.8076\n",
      "Epoch 4/10\n",
      "8083/8083 [==============================] - 2s - loss: 0.3494 - acc: 0.9209 - val_loss: 0.8534 - val_acc: 0.8065\n",
      "Epoch 5/10\n",
      "8083/8083 [==============================] - 2s - loss: 0.2681 - acc: 0.9364 - val_loss: 0.8490 - val_acc: 0.8187\n",
      "Epoch 6/10\n",
      "8083/8083 [==============================] - 2s - loss: 0.2170 - acc: 0.9470 - val_loss: 0.8693 - val_acc: 0.8187\n",
      "Epoch 7/10\n",
      "8083/8083 [==============================] - 2s - loss: 0.1838 - acc: 0.9535 - val_loss: 0.9196 - val_acc: 0.8109\n",
      "Epoch 8/10\n",
      "8083/8083 [==============================] - 2s - loss: 0.1588 - acc: 0.9557 - val_loss: 0.9158 - val_acc: 0.8131\n",
      "Epoch 9/10\n",
      "8083/8083 [==============================] - 2s - loss: 0.1512 - acc: 0.9567 - val_loss: 0.9297 - val_acc: 0.8120\n",
      "Epoch 10/10\n",
      "8083/8083 [==============================] - 2s - loss: 0.1501 - acc: 0.9588 - val_loss: 0.9330 - val_acc: 0.7987\n",
      "2176/2246 [============================>.] - ETA: 0s\n",
      "Test score: 0.906570495502\n",
      "Test accuracy: 80.7212822371\n"
     ]
    }
   ],
   "source": [
    "batch_size =128\n",
    "epochs = 10\n",
    "\n",
    "print('Building model...')\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(MAX_NB_WORDS,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)\n",
    "\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('\\nTest score:', score[0])\n",
    "print('Test accuracy:', score[1]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Words Embedding and Keras Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train size (8982,)\n",
      "y_train size (8982,)\n",
      "x_test size (2246,)\n",
      "y_test size (2246,)\n",
      "\n",
      "[1, 3267, 699, 3434, 2295, 56, 16784, 7511, 9, 56, 3906, 1073, 81, 5, 1198, 57, 366, 737, 132, 20, 4093, 7, 19261, 49, 2295, 13415, 1037, 3267, 699, 3434, 8, 7, 10, 241, 16, 855, 129, 231, 783, 5, 4, 587, 2295, 13415, 30625, 775, 7, 48, 34, 191, 44, 35, 1795, 505, 17, 12]\n",
      "Vectorizing sequence data...\n",
      "x_train shape: (8982, 100)\n",
      "x_test shape: (2246, 100)\n",
      "Convert class vector to binary class matrix (for use with categorical_crossentropy)\n",
      "y_train shape: (8982, 46)\n",
      "y_test shape: (2246, 46)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(path=\"reuters.npz\",\n",
    "                                                         num_words=None,\n",
    "                                                         skip_top=0,\n",
    "                                                         maxlen=None,\n",
    "                                                         test_split=0.2,\n",
    "                                                         seed=113,\n",
    "                                                         start_char=1,\n",
    "                                                         oov_char=2,\n",
    "                                                         index_from=3)\n",
    "\n",
    "print(\"x_train size\", x_train.shape)\n",
    "print(\"y_train size\", y_train.shape)\n",
    "print(\"x_test size\", x_test.shape)\n",
    "print(\"y_test size\", y_test.shape)\n",
    "print(\"\")\n",
    "print(x_train[1])\n",
    "# print(\"Number of classes:\", np.unique(y_train).shape[0])\n",
    "# word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
    "\n",
    "\n",
    "MAX_NB_WORDS = 100\n",
    "\n",
    "num_classes = np.unique(y_train).shape[0]\n",
    "print('Vectorizing sequence data...')\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
    "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Convert class vector to binary class matrix '\n",
    "      '(for use with categorical_crossentropy)')\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 0\n",
      " 0 0 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./embedding/glove.6B.zip'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding/glove.6B.100d.txt\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 400000 word vectors in Glove 6B 100d.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30980, 100)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Train on 8083 samples, validate on 899 samples\n",
      "Epoch 1/10\n",
      "8083/8083 [==============================] - 4s - loss: 1.8250 - acc: 0.5941 - val_loss: 1.4371 - val_acc: 0.6741\n",
      "Epoch 2/10\n",
      "8083/8083 [==============================] - 4s - loss: 1.3959 - acc: 0.6646 - val_loss: 1.3986 - val_acc: 0.6774\n",
      "Epoch 3/10\n",
      "8083/8083 [==============================] - 4s - loss: 1.3152 - acc: 0.6754 - val_loss: 1.3759 - val_acc: 0.6808\n",
      "Epoch 4/10\n",
      "8083/8083 [==============================] - 3s - loss: 1.2730 - acc: 0.6799 - val_loss: 1.3415 - val_acc: 0.6796\n",
      "Epoch 5/10\n",
      "8083/8083 [==============================] - 3s - loss: 1.2060 - acc: 0.6926 - val_loss: 1.3074 - val_acc: 0.6908\n",
      "Epoch 6/10\n",
      "8083/8083 [==============================] - 4s - loss: 1.1833 - acc: 0.7004 - val_loss: 1.2919 - val_acc: 0.7019\n",
      "Epoch 7/10\n",
      "8083/8083 [==============================] - 4s - loss: 1.1513 - acc: 0.7046 - val_loss: 1.2995 - val_acc: 0.6908\n",
      "Epoch 8/10\n",
      "8083/8083 [==============================] - 4s - loss: 1.1136 - acc: 0.7077 - val_loss: 1.3104 - val_acc: 0.6897\n",
      "Epoch 9/10\n",
      "8083/8083 [==============================] - 4s - loss: 1.0795 - acc: 0.7142 - val_loss: 1.3125 - val_acc: 0.6986\n",
      "Epoch 10/10\n",
      "8083/8083 [==============================] - 4s - loss: 1.0515 - acc: 0.7228 - val_loss: 1.3196 - val_acc: 0.6885\n",
      "2048/2246 [==========================>...] - ETA: 0s\n",
      "Test score: 1.28732361808\n",
      "Test accuracy: 68.6999110271\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
